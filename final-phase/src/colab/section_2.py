# -*- coding: utf-8 -*-
"""section_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yw74ut4DXkRnQCaPvjiy9ryL3rEVsHn3
"""

import nltk
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

data_path = '/content/drive/MyDrive/University/NLP/Final-Project/data'
# data_path = '/content/drive/MyDrive/data'

train_data_path = f'{data_path}/train'
true_train_path = f'{train_data_path}/true.csv'
false_train_path = f'{train_data_path}/false.csv'

test_data_path = f'{data_path}/test'
true_test_path = f'{test_data_path}/true.csv'
false_test_path = f'{test_data_path}/false.csv'

# Pytorch's nn module has lots of useful feature
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from nltk.tokenize import sent_tokenize, word_tokenize
from torch.nn.utils.rnn import pad_sequence
import csv

def pad_batched_sequence(batch):
    tweets = []
    tweets_lengths = []
    labels = []
    for (tweet, label) in batch:
      tweets.append(torch.tensor(tweet).cuda())
      tweets_lengths.append(len(tweet))
      labels.append(label)

    tweets = pad_sequence(tweets, padding_value=0, batch_first=True).cuda()
    tweets_lengths = torch.tensor(tweets_lengths).cuda()
    labels = torch.tensor(labels).cuda()
    return tweets, tweets_lengths, labels

word_to_idx = {
  '<pad>': 0,
  '<start>': 1,
  '<stop>': 2
}
class SpellCheckingDataset(Dataset):

    def __init__(self, data_paths, labels, batch_size=32):
        self.dataset = []
        
        idx = 3
        for i in range(len(data_paths)):
          data_path = data_paths[i]
          with open(data_path, 'r', encoding='utf-8') as file:
              data = csv.reader(file)
              for item in data:
                tokenized_tweet = ['<start>'] + word_tokenize(item[0]) + ['<stop>']
                for word in tokenized_tweet:
                  if (word not in word_to_idx):
                    word_to_idx[word] = idx
                    idx += 1
                self.dataset.append((tokenized_tweet, labels[i]))
        self.batch_size = batch_size
        self.vocab_size = len(word_to_idx)

    def __getitem__(self, idx):
        return [word_to_idx[w] for w in self.dataset[idx][0]], self.dataset[idx][1]

    def __len__(self):
      return len(self.dataset)

class LSTMNet(nn.Module):
    
    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):
        
        super(LSTMNet,self).__init__()
        
        # Embedding layer converts integer sequences to vector sequences
        self.embedding = nn.Embedding(vocab_size,embedding_dim)
        
        # LSTM layer process the vector sequences 
        self.lstm = nn.LSTM(embedding_dim,
                            hidden_dim,
                            num_layers = n_layers,
                            bidirectional = bidirectional,
                            dropout = dropout,
                            batch_first = True
                           )
        
        # Dense layer to predict 
        self.fc = nn.Linear(hidden_dim * 2,output_dim)
        # Prediction activation function
        self.sigmoid = nn.Sigmoid()
        
    
    def forward(self,text,text_lengths):
        embedded = self.embedding(text)
        
        # Thanks to packing, LSTM don't see padding tokens 
        # and this makes our model better
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True, enforce_sorted=False)
        
        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)
        
        # Concatenating the final forward and backward hidden states
        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)
        
        dense_outputs=self.fc(hidden)

        #Final activation function
        outputs=self.sigmoid(dense_outputs)
        
        return outputs

true_label = 1
false_label = 0

train_dataset = SpellCheckingDataset([true_train_path, false_train_path], 
                                          [true_label, false_label])

test_dataset = SpellCheckingDataset([true_test_path, false_test_path], 
                                          [true_label, false_label])

SIZE_OF_VOCAB = train_dataset.vocab_size
EMBEDDING_DIM = 100
NUM_HIDDEN_NODES = 64
NUM_OUTPUT_NODES = 1
NUM_LAYERS = 2
BIDIRECTION = True
DROPOUT = 0.2

model = LSTMNet(SIZE_OF_VOCAB,
                EMBEDDING_DIM,
                NUM_HIDDEN_NODES,
                NUM_OUTPUT_NODES,
                NUM_LAYERS,
                BIDIRECTION,
                DROPOUT
               )

import torch.optim as optim
model = model.cuda()
optimizer = optim.Adam(model.parameters(),lr=1e-4)
criterion = nn.BCELoss()
criterion = criterion.cuda()

def binary_accuracy(preds, y):
    #round predictions to the closest integer
    rounded_preds = torch.round(preds)
    
    correct = (rounded_preds == y).float() 
    acc = correct.sum() / len(correct)
    return acc

def train(model,iterator,optimizer,criterion):
    
    epoch_loss = 0.0
    epoch_acc = 0.0
    
    model.train()
    
    for batch in iterator:
        
        # cleaning the cache of optimizer
        optimizer.zero_grad()
        
        text,text_lengths, labels = batch
        
        # forward propagation and squeezing
        predictions = model(text,text_lengths).squeeze()
        
        # computing loss / backward propagation
        loss = criterion(predictions.double(),labels.double())
        loss.backward()
        
        # accuracy
        acc = binary_accuracy(predictions,labels)
        
        # updating params
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
    # It'll return the means of loss and accuracy
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def evaluate(model,iterator,criterion):
    
    epoch_loss = 0.0
    epoch_acc = 0.0
    
    # deactivate the dropouts
    model.eval()
    
    # Sets require_grad flat False
    with torch.no_grad():
        for batch in iterator:
            text,text_lengths, labels = batch
            
            predictions = model(text,text_lengths).squeeze()
              
            #compute loss and accuracy
            loss = criterion(predictions.double(), labels.double())
            acc = binary_accuracy(predictions, labels)
            
            #keep track of loss and accuracy
            epoch_loss += loss.item()
            epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

from torch.utils.data.dataloader import DataLoader

EPOCH_NUMBER = 15
train_loader = DataLoader(train_dataset, 
                          batch_size=32, 
                          shuffle=True,
                          drop_last=True, 
                          collate_fn=pad_batched_sequence)

test_loader = DataLoader(test_dataset, 
                          batch_size=32, 
                          shuffle=True,
                          drop_last=True, 
                          collate_fn=pad_batched_sequence)

for epoch in range(1,EPOCH_NUMBER+1):
    
    train_loss,train_acc = train(model,train_loader,optimizer,criterion)
    
    valid_loss,valid_acc = evaluate(model,test_loader,criterion)
    
    # Showing statistics
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')
    print()


{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "section_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KYETeqqTQxq",
        "outputId": "4e5950dd-ef9f-4a0d-b867-e77e7bef1999"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/University/NLP/Final-Project/data'\n",
        "# data_path = '/content/drive/MyDrive/data'\n",
        "\n",
        "train_data_path = f'{data_path}/train'\n",
        "true_train_path = f'{train_data_path}/true.csv'\n",
        "false_train_path = f'{train_data_path}/false.csv'\n",
        "\n",
        "test_data_path = f'{data_path}/test'\n",
        "true_test_path = f'{test_data_path}/true.csv'\n",
        "false_test_path = f'{test_data_path}/false.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cb6-ij9SgvY",
        "outputId": "863c52d0-b8bc-4be5-cedb-af0555e6288e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch's nn module has lots of useful feature\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import csv\n",
        "\n",
        "def pad_batched_sequence(batch):\n",
        "    tweets = []\n",
        "    tweets_lengths = []\n",
        "    labels = []\n",
        "    for (tweet, label) in batch:\n",
        "      tweets.append(torch.tensor(tweet).cuda())\n",
        "      tweets_lengths.append(len(tweet))\n",
        "      labels.append(label)\n",
        "\n",
        "    tweets = pad_sequence(tweets, padding_value=0, batch_first=True).cuda()\n",
        "    tweets_lengths = torch.tensor(tweets_lengths).cuda()\n",
        "    labels = torch.tensor(labels).cuda()\n",
        "    return tweets, tweets_lengths, labels\n",
        "\n",
        "word_to_idx = {\n",
        "  '<pad>': 0,\n",
        "  '<start>': 1,\n",
        "  '<stop>': 2\n",
        "}\n",
        "class SpellCheckingDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_paths, labels, batch_size=32):\n",
        "        self.dataset = []\n",
        "        \n",
        "        idx = 3\n",
        "        for i in range(len(data_paths)):\n",
        "          data_path = data_paths[i]\n",
        "          with open(data_path, 'r', encoding='utf-8') as file:\n",
        "              data = csv.reader(file)\n",
        "              for item in data:\n",
        "                tokenized_tweet = ['<start>'] + word_tokenize(item[0]) + ['<stop>']\n",
        "                for word in tokenized_tweet:\n",
        "                  if (word not in word_to_idx):\n",
        "                    word_to_idx[word] = idx\n",
        "                    idx += 1\n",
        "                self.dataset.append((tokenized_tweet, labels[i]))\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size = len(word_to_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [word_to_idx[w] for w in self.dataset[idx][0]], self.dataset[idx][1]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dataset)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    \n",
        "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
        "        \n",
        "        super(LSTMNet,self).__init__()\n",
        "        \n",
        "        # Embedding layer converts integer sequences to vector sequences\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "        \n",
        "        # LSTM layer process the vector sequences \n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = n_layers,\n",
        "                            bidirectional = bidirectional,\n",
        "                            dropout = dropout,\n",
        "                            batch_first = True\n",
        "                           )\n",
        "        \n",
        "        # Dense layer to predict \n",
        "        self.fc = nn.Linear(hidden_dim * 2,output_dim)\n",
        "        # Prediction activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    \n",
        "    def forward(self,text,text_lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        # Thanks to packing, LSTM don't see padding tokens \n",
        "        # and this makes our model better\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)\n",
        "        \n",
        "        # Concatenating the final forward and backward hidden states\n",
        "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
        "        \n",
        "        dense_outputs=self.fc(hidden)\n",
        "\n",
        "        #Final activation function\n",
        "        outputs=self.sigmoid(dense_outputs)\n",
        "        \n",
        "        return outputs\n",
        "    "
      ],
      "metadata": {
        "trusted": true,
        "id": "OL0CeKtRBKYn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "true_label = 1\n",
        "false_label = 0\n",
        "\n",
        "train_dataset = SpellCheckingDataset([true_train_path, false_train_path], \n",
        "                                          [true_label, false_label])\n",
        "\n",
        "test_dataset = SpellCheckingDataset([true_test_path, false_test_path], \n",
        "                                          [true_label, false_label])\n",
        "\n",
        "SIZE_OF_VOCAB = train_dataset.vocab_size\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_HIDDEN_NODES = 64\n",
        "NUM_OUTPUT_NODES = 1\n",
        "NUM_LAYERS = 2\n",
        "BIDIRECTION = True\n",
        "DROPOUT = 0.2"
      ],
      "metadata": {
        "trusted": true,
        "id": "tHdNQyocBKYo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMNet(SIZE_OF_VOCAB,\n",
        "                EMBEDDING_DIM,\n",
        "                NUM_HIDDEN_NODES,\n",
        "                NUM_OUTPUT_NODES,\n",
        "                NUM_LAYERS,\n",
        "                BIDIRECTION,\n",
        "                DROPOUT\n",
        "               )\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "duYyBIzqBKYp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "model = model.cuda()\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
        "criterion = nn.BCELoss()\n",
        "criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "H2WtRT_VTrxX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    \n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "3VNrDO6ITxSt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,iterator,optimizer,criterion):\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # cleaning the cache of optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text,text_lengths, labels = batch\n",
        "        \n",
        "        # forward propagation and squeezing\n",
        "        predictions = model(text,text_lengths).squeeze()\n",
        "        \n",
        "        # computing loss / backward propagation\n",
        "        loss = criterion(predictions.double(),labels.double())\n",
        "        loss.backward()\n",
        "        \n",
        "        # accuracy\n",
        "        acc = binary_accuracy(predictions,labels)\n",
        "        \n",
        "        # updating params\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    # It'll return the means of loss and accuracy\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "        "
      ],
      "metadata": {
        "id": "TInRo66fT2FS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model,iterator,criterion):\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    \n",
        "    # deactivate the dropouts\n",
        "    model.eval()\n",
        "    \n",
        "    # Sets require_grad flat False\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text,text_lengths, labels = batch\n",
        "            \n",
        "            predictions = model(text,text_lengths).squeeze()\n",
        "              \n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions.double(), labels.double())\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            \n",
        "            #keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "8T-r-NdhT2yJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "EPOCH_NUMBER = 15\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=32, \n",
        "                          shuffle=True,\n",
        "                          drop_last=True, \n",
        "                          collate_fn=pad_batched_sequence)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, \n",
        "                          batch_size=32, \n",
        "                          shuffle=True,\n",
        "                          drop_last=True, \n",
        "                          collate_fn=pad_batched_sequence)\n",
        "\n",
        "for epoch in range(1,EPOCH_NUMBER+1):\n",
        "    \n",
        "    train_loss,train_acc = train(model,train_loader,optimizer,criterion)\n",
        "    \n",
        "    valid_loss,valid_acc = evaluate(model,test_loader,criterion)\n",
        "    \n",
        "    # Showing statistics\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    print()"
      ],
      "metadata": {
        "id": "gYgGYsiFT5S4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93d7d79-cb8a-4fdb-d7dc-87007154ee35"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.692 | Train Acc: 52.11%\n",
            "\t Val. Loss: 0.692 |  Val. Acc: 51.88%\n",
            "\n",
            "\tTrain Loss: 0.669 | Train Acc: 58.39%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 53.89%\n",
            "\n",
            "\tTrain Loss: 0.632 | Train Acc: 63.48%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 55.10%\n",
            "\n",
            "\tTrain Loss: 0.589 | Train Acc: 68.44%\n",
            "\t Val. Loss: 0.714 |  Val. Acc: 56.64%\n",
            "\n",
            "\tTrain Loss: 0.542 | Train Acc: 72.23%\n",
            "\t Val. Loss: 0.740 |  Val. Acc: 57.96%\n",
            "\n",
            "\tTrain Loss: 0.497 | Train Acc: 75.29%\n",
            "\t Val. Loss: 0.743 |  Val. Acc: 59.78%\n",
            "\n",
            "\tTrain Loss: 0.457 | Train Acc: 78.12%\n",
            "\t Val. Loss: 0.779 |  Val. Acc: 59.41%\n",
            "\n",
            "\tTrain Loss: 0.414 | Train Acc: 80.82%\n",
            "\t Val. Loss: 0.835 |  Val. Acc: 59.97%\n",
            "\n",
            "\tTrain Loss: 0.380 | Train Acc: 82.61%\n",
            "\t Val. Loss: 0.869 |  Val. Acc: 60.98%\n",
            "\n",
            "\tTrain Loss: 0.344 | Train Acc: 84.58%\n",
            "\t Val. Loss: 0.956 |  Val. Acc: 60.26%\n",
            "\n",
            "\tTrain Loss: 0.310 | Train Acc: 86.54%\n",
            "\t Val. Loss: 0.977 |  Val. Acc: 62.02%\n",
            "\n",
            "\tTrain Loss: 0.278 | Train Acc: 88.12%\n",
            "\t Val. Loss: 1.016 |  Val. Acc: 61.99%\n",
            "\n",
            "\tTrain Loss: 0.249 | Train Acc: 89.44%\n",
            "\t Val. Loss: 1.097 |  Val. Acc: 61.85%\n",
            "\n",
            "\tTrain Loss: 0.221 | Train Acc: 90.92%\n",
            "\t Val. Loss: 1.139 |  Val. Acc: 62.31%\n",
            "\n",
            "\tTrain Loss: 0.197 | Train Acc: 92.14%\n",
            "\t Val. Loss: 1.231 |  Val. Acc: 61.68%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TeAddTLcT8Ff"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "RRyLrdao0VQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6353e97e-9af7-4fc6-caa5-d0a0c25bb12f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import csv\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torchmetrics import Accuracy\n",
        "import os\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "pcY4HNIxValV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data'\n",
        "\n",
        "train_data_path = f'{data_path}/train'\n",
        "true_train_path = f'{train_data_path}/true.csv'\n",
        "false_train_path = f'{train_data_path}/false.csv'\n",
        "\n",
        "test_data_path = f'{data_path}/test'\n",
        "true_test_path = f'{test_data_path}/true.csv'\n",
        "false_test_path = f'{test_data_path}/false.csv'"
      ],
      "metadata": {
        "id": "TZ0ZIOuCLJbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2244177-9c9a-4ff8-9536-9177da846294"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim=768, num_class=1):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
        "        self.fc = nn.Linear(embed_dim, num_class).cuda()\n",
        "        self.sigmoid = nn.Sigmoid().cuda()\n",
        "\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        inputs = {\n",
        "            \"input_ids\": input_ids.cuda(), \n",
        "            \"attention_mask\" : attention_masks.cuda()\n",
        "        }\n",
        "        outputs = self.bert(**inputs)\n",
        "        output = outputs.last_hidden_state\n",
        "        output = output[:,0,:]\n",
        "        output = self.fc(output)\n",
        "        return self.sigmoid(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "U4XLqdtW8rN5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpellCheckingDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, data_paths, labels, batch_size=32):\n",
        "        self.dataset = []\n",
        "        for i in range(len(data_paths)):\n",
        "          data_path = data_paths[i]\n",
        "          with open(data_path, 'r', encoding='utf-8') as file:\n",
        "              data = csv.reader(file)\n",
        "              for item in data:\n",
        "                self.dataset.append((item[0], labels[i]))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if (self.tokenizer == None):\n",
        "          raise Exception('Tokenizer cannot be null')\n",
        "\n",
        "        tweet, label = self.dataset[idx]\n",
        "        tokenized_tweet = self.tokenizer(tweet[0])\n",
        "        input_ids = tokenized_tweet['input_ids']\n",
        "        attention_mask = tokenized_tweet['attention_mask']\n",
        "\n",
        "        return input_ids, attention_mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dataset)"
      ],
      "metadata": {
        "id": "f0PokHctSoa0"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINTS_DIR = '/content/models'\n",
        "\n",
        "def pad_batched_sequence(batch):\n",
        "    \n",
        "    input_ids = [torch.tensor(item[0]) for item in batch]\n",
        "    # print('input ids')\n",
        "    # print(input_ids)\n",
        "\n",
        "    attention_masks =  [torch.tensor(item[1]) for item in batch]\n",
        "    # print('attention masks')\n",
        "    # print(attention_masks)\n",
        "\n",
        "    input_ids = pad_sequence(input_ids, padding_value=0, batch_first=True)\n",
        "    # print('pad input ids')\n",
        "    # print(input_ids)\n",
        "\n",
        "    attention_masks = pad_sequence(attention_masks, \n",
        "                                   padding_value=0, \n",
        "                                   batch_first=True)\n",
        "    # print('pad attention masks')\n",
        "    # print(attention_masks)\n",
        "\n",
        "    labels = None\n",
        "    \n",
        "    if batch[0][2]:\n",
        "        labels = torch.tensor([[item[2]] for item in batch]).double().cuda()\n",
        "    \n",
        "    return input_ids.cuda(), attention_masks.cuda(), labels\n",
        "\n",
        "class SpellCheckingTrainer():\n",
        "\n",
        "  def __init__(self,\n",
        "               model,\n",
        "               train_dataset,\n",
        "               save_data_path,\n",
        "               batch_size=32,\n",
        "               epochs=20,\n",
        "               lr=0.001):\n",
        "        \n",
        "    self.model = model\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.train_loader = DataLoader(train_dataset, \n",
        "                                   batch_size=batch_size, \n",
        "                                   shuffle=True, \n",
        "                                   drop_last=True, \n",
        "                                   collate_fn=pad_batched_sequence)\n",
        "    self.save_path = save_data_path\n",
        "    self.loss_function = nn.BCELoss()\n",
        "    self.optimizer = torch.optim.Adam(list(self.model.parameters()), lr=lr)        \n",
        "    self.accuracy = Accuracy(num_classes=1)\n",
        "\n",
        "  def train_one_epoch(self, epoch_index):\n",
        "    running_loss = 0.\n",
        "    running_accuracy = 0.\n",
        "    last_loss = 0.\n",
        "    threshold = torch.tensor([0.5]).cuda()\n",
        "\n",
        "    for i, data in tqdm.tqdm(enumerate(self.train_loader), \n",
        "                             total=len(self.train_loader)):\n",
        "        # Every data instance is an input + label pair\n",
        "        input_ids, attention_masks, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = self.model(input_ids, attention_masks)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = self.loss_function(outputs.double(), labels.double())\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        result = (outputs > threshold).float() * 1\n",
        "        running_accuracy += torch.sum(result == labels) / self.batch_size\n",
        "        \n",
        "        if i % 10 == 9:\n",
        "            last_loss = running_loss / 10 # loss per batch\n",
        "            last_accuracy = running_accuracy / 10 # accuracy per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            print('  batch {} accuracy: {}'.format(i + 1, last_accuracy))\n",
        "            running_loss = 0.\n",
        "            running_accuracy = 0.\n",
        "\n",
        "    return last_loss\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    epoch_number = 0\n",
        "    for epoch in range(self.epochs):\n",
        "        print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        self.model.train(True)\n",
        "        avg_loss = self.train_one_epoch(epoch_number)\n",
        "\n",
        "        # We don't need gradients on to do reporting\n",
        "        self.model.train(False)\n",
        "\n",
        "        epoch_number += 1\n",
        "    \n",
        "    torch.save(self.model.state_dict(), self.save_path)\n",
        "    \n",
        "          \n",
        "          "
      ],
      "metadata": {
        "id": "jByNbykLSq_3"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "true_label = 1\n",
        "false_label = 0\n",
        "\n",
        "true_train_dataset = SpellCheckingDataset(tokenizer, [true_train_path], [true_label])\n",
        "false_train_dataset = SpellCheckingDataset(tokenizer, [false_train_path], [false_label])\n",
        "\n",
        "test_dataset = SpellCheckingDataset(tokenizer, \n",
        "                                    [true_test_path, false_test_path],\n",
        "                                    [true_label, false_label])\n",
        "\n",
        "true_saved_model_path = '/content/drive/MyDrive/models/true_bert.berm_lm'\n",
        "false_saved_model_path = '/content/drive/MyDrive/models/false_bert.berm_lm'\n",
        "\n",
        "true_trainer = SpellCheckingTrainer(TextClassificationModel(), \n",
        "                                    true_train_dataset,\n",
        "                                    true_saved_model_path)\n",
        "\n",
        "false_trainer = SpellCheckingTrainer(TextClassificationModel(), \n",
        "                                     false_train_dataset,\n",
        "                                     false_saved_model_path)\n",
        " "
      ],
      "metadata": {
        "id": "hlku2sOhrGsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012fc769-7e4f-4358-a63b-b44f80909a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_trainer.train()\n",
        "false_trainer.train()\n",
        "\n",
        "# model = TextClassificationModel()\n",
        "# model.load_state_dict(torch.load(true_saved_model_path))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "JqciaevS-GSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}